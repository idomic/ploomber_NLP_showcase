{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NRxx3uIM2BC_",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install hackernews-client transformers kaggle catalyst  -q "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 135
    },
    "id": "-4XHez7rrTtc",
    "outputId": "6f2b8eb9-2972-460f-d63d-5cf523b23f2e"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/gdrive')\n",
    "%cd /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BSW4jHV82P5T"
   },
   "outputs": [],
   "source": [
    "!kaggle datasets download -d rmisra/news-category-dataset > \"error\"\n",
    "!cp '/gdrive/My Drive/kaggle.json' /root/.kaggle/\n",
    "!kaggle datasets download -d rmisra/news-category-dataset\n",
    "!git clone https://github.com/huggingface/transformers;cd transformers; pip install -q .\n",
    "!unzip news-category-dataset.zip\n",
    "!pip install -q catalyst\n",
    "!git clone https://github.com/mhjabreel/CharCnn_Keras\n",
    "!wget https://www.cs.umb.edu/~smimarog/textmining/datasets/r8-train-all-terms.txt\n",
    "!wget https://www.cs.umb.edu/~smimarog/textmining/datasets/r8-test-all-terms.txt\n",
    "!wget https://www.cs.umb.edu/~smimarog/textmining/datasets/r52-train-all-terms.txt\n",
    "!wget https://www.cs.umb.edu/~smimarog/textmining/datasets/r52-test-all-terms.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 267
    },
    "id": "k9xjT5J13YNs",
    "outputId": "11c53cbd-d1cf-45df-cc20-d87c5b1d7e13"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import requests\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "# Core\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Data Visualization\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "import matplotlib.pyplot as plt\n",
    "# SKlearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, f1_score\n",
    "# PyTorch \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# Transformers \n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
    "# Catalyst\n",
    "from catalyst.dl import SupervisedRunner\n",
    "from catalyst.dl.callbacks import AccuracyCallback, F1ScoreCallback, OptimizerCallback\n",
    "from catalyst.dl.callbacks import CheckpointCallback, InferCallback\n",
    "from catalyst.utils import set_global_seed, prepare_cudnn\n",
    "# Extras\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import logging\n",
    "from typing import Mapping, List\n",
    "import datetime as dt\n",
    "\n",
    "\n",
    "from IPython.display import HTML, Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rXri8CekrEOn"
   },
   "outputs": [],
   "source": [
    "d0 = pd.read_json(\"News_Category_Dataset_v2.json\",lines=True)\n",
    "df = d0[['category']]\n",
    "df['text'] = (d0['headline'] + \" \" + d0['short_description']).str.lower()\n",
    "\n",
    "reduced_cats = {\"CULTURE & ARTS\":[\"ARTS\",\"ARTS & CULTURE\",\"CULTURE & ARTS\"],\n",
    " \"BUSINESS\": [\"BUSINESS\"],\n",
    " \"MONEY\":[\"MONEY\"],\n",
    " \"EDUCATION\": [\"EDUCATION\",\"COLLEGE\"],\n",
    " \"COMEDY & ENTERTAINMENT\" : [\"COMEDY\",\"ENTERTAINMENT\",\"MEDIA\"],\n",
    " \"FAMILY\" : [\"PARENTING\",\"DIVORCE\",\"WEDDINGS\"],\n",
    " \"OPINION\": [\"BLACK VOICES\",\"LATINO VOICES\",\"QUEER VOICES\",\"WOMEN\"],\n",
    " \"FOOD & TASTE\" : [\"FOOD & DRINK\",\"TASTE\"],\n",
    " \"HEALTH & LIVING\": [\"WELLNESS\",\"HEALTHY LIVING\",\"STYLE & BEAUTY\",\"HOME & LIVING\",\"STYLE\"],\n",
    " \"RELIGION\" : [\"RELIGION\"],\n",
    " \"POLITICS\" : [\"POLITICS\"],\n",
    " \"SPORTS\" : [\"SPORTS\"],\n",
    " \"TRAVEL\" : [\"TRAVEL\"],\n",
    " \"NEWS\" :[\"GOOD NEWS\",\"WORLDPOST\",\"WORLD NEWS\",\"WEIRD NEWS\"],\n",
    " \"ENVIRONMENT\" : [\"GREEN\",\"ENVIRONMENT\"],\n",
    " \"CRIME\" :[\"CRIME\"],\n",
    " \"SCIENCE\": [\"SCIENCE\"],\n",
    " \"TECH\": [\"TECH\"]\n",
    " }\n",
    "flat_list = [item for sublist in reduced_cats.values() for item in sublist]\n",
    "df = df[df['category'].isin(flat_list)]\n",
    "df['category'] = df['category'].map({item:cat for cat in reduced_cats for item in reduced_cats[cat]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eU7uErqFRJdf"
   },
   "outputs": [],
   "source": [
    "#https://github.com/arimbr/valohai-fasttext-example\n",
    "REDDIT_S3_PATH = 'https://valohai-fasttext-example.s3.eu-west-3.amazonaws.com/reddit/'\n",
    "\n",
    "posts_df = pd.read_csv(REDDIT_S3_PATH + 'reddit_posts_4M.csv')\n",
    "predictions_df = pd.read_csv(REDDIT_S3_PATH + 'test_predictions.csv')\n",
    "\n",
    "posts_df['date'] = posts_df['created_utc'].apply(datetime.fromtimestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TguR3B2e5Zcz"
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "LOG_DIR = \"./news-classification\"    \n",
    "NUM_EPOCHS = 4                         \n",
    "BATCH_SIZE = 40                        \n",
    "MAX_SEQ_LENGTH = 256                   \n",
    "LEARN_RATE = 5e-5                      \n",
    "ACCUM_STEPS = 4                        \n",
    "SEED = 42                              \n",
    "FP16_PARAMS = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MRjh7sYb3aIF"
   },
   "outputs": [],
   "source": [
    "text = pd.DataFrame({\n",
    "    \"text\" : df.text,\n",
    "    \"label\" : df.category  \n",
    "})\n",
    "\n",
    "train, val = train_test_split(\n",
    "    text, test_size=0.30, random_state=SEED)\n",
    "\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "val.reset_index(drop=True, inplace=True)\n",
    "\n",
    "val, test = train_test_split(\n",
    "    val, test_size=0.10, random_state=SEED)\n",
    "\n",
    "val.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "test_true = pd.DataFrame({\n",
    "    \"label\" : test.label\n",
    "})\n",
    "\n",
    "test.drop(columns=[\"label\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cfel9kDPsN5v"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 114,
     "referenced_widgets": [
      "b9973edfe3d1403b8c39bbbabbd68b29",
      "0705738069b84a8ca18ecb356a1af95e",
      "ccd9a5561e7c432b88b9a1cb0045f2df",
      "abc45af2ca2247f79058a564b4ca0f9c",
      "ced4b56badec4439bed234f178839976",
      "bf695498a04841d2a3c59e902a9d28ad",
      "217fea16e233406991959a5b7fcaa11f",
      "bf1bce0302d045b5b623e065ecbe333e",
      "e633c9bf5e6344d79249bd5ee52168d8",
      "12381c45fea6481cbdb04b45c873139c",
      "d89a08fe12b0445ea2e83e16a03689c9",
      "397a033118ca4fa69724891352adb5bb",
      "996e6d1b89404e83b87bab40dd456b97",
      "d007f141bf3947999aa1f47e57013552",
      "d875ced536cd4201b1d70976262f74fc",
      "b90acff09d264e7da1d235dfeac0b3aa"
     ]
    },
    "id": "lHD0BQgB3cJs",
    "outputId": "696ec3ac-20c5-4ef4-f3be-e94a0a76209d"
   },
   "outputs": [],
   "source": [
    "class TextClassificationDataset(Dataset):\n",
    "\n",
    "    def __init__(self,\n",
    "                 texts: List[str],\n",
    "                 labels: List[str] = None,\n",
    "                 label_dict: Mapping[str, int] = None,\n",
    "                 max_seq_length: int = 512,\n",
    "                 model_name: str = 'distilbert-base-uncased'):\n",
    "\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.label_dict = label_dict\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "        if self.label_dict is None and labels is not None:\n",
    "            self.label_dict = dict(zip(sorted(set(labels)),\n",
    "                                       range(len(set(labels)))))\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        logging.getLogger(\n",
    "            \"transformers.tokenization_utils\").setLevel(logging.FATAL)\n",
    "\n",
    "        self.sep_vid = self.tokenizer.vocab[\"[SEP]\"]\n",
    "        self.cls_vid = self.tokenizer.vocab[\"[CLS]\"]\n",
    "        self.pad_vid = self.tokenizer.vocab[\"[PAD]\"]\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, index) -> Mapping[str, torch.Tensor]:\n",
    "\n",
    "        x = self.texts[index]\n",
    "        x_encoded = self.tokenizer.encode(\n",
    "            x,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_seq_length,\n",
    "            return_tensors=\"pt\",\n",
    "        ).squeeze(0)\n",
    "\n",
    "        true_seq_length = x_encoded.size(0)\n",
    "        pad_size = self.max_seq_length - true_seq_length\n",
    "        pad_ids = torch.Tensor([self.pad_vid] * pad_size).long()\n",
    "        x_tensor = torch.cat((x_encoded, pad_ids))\n",
    "\n",
    "        mask = torch.ones_like(x_encoded, dtype=torch.int8)\n",
    "        mask_pad = torch.zeros_like(pad_ids, dtype=torch.int8)\n",
    "        mask = torch.cat((mask, mask_pad))\n",
    "\n",
    "        output_dict = {\n",
    "            \"features\": x_tensor,\n",
    "            'attention_mask': mask\n",
    "        }\n",
    "\n",
    "        if self.labels is not None:\n",
    "            y = self.labels[index]\n",
    "            y_encoded = torch.Tensor(\n",
    "                [self.label_dict.get(y, -1)]\n",
    "            ).long().squeeze(0)\n",
    "            output_dict[\"targets\"] = y_encoded\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "train_dataset = TextClassificationDataset(\n",
    "    texts=train['text'].values.tolist(),\n",
    "    labels=train['label'].values.tolist(),\n",
    "    label_dict=None,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    model_name=MODEL_NAME\n",
    ")\n",
    "\n",
    "valid_dataset = TextClassificationDataset(\n",
    "    texts=val['text'].values.tolist(),\n",
    "    labels=val['label'].values.tolist(),\n",
    "    label_dict=train_dataset.label_dict,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    model_name=MODEL_NAME\n",
    ")\n",
    "\n",
    "test_dataset = TextClassificationDataset(\n",
    "    texts=test['text'].values.tolist(),\n",
    "    labels=None,\n",
    "    label_dict=None,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    model_name=MODEL_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "OYte-WOcsZI3",
    "outputId": "3b2bca7e-9617-4f0a-eff8-26c55d477094"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json.dumps({v:k for k,v in train_dataset.label_dict.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "6cee380d182840e5a3d877ebdc86c7de"
     ]
    },
    "id": "TJ6aUPws4Mht",
    "outputId": "46072dfa-2ff5-4b65-f769-3bdcdd76eba3"
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = len(train_dataset.label_dict)\n",
    "train_val_loaders = {\n",
    "    \"train\": DataLoader(dataset=train_dataset,\n",
    "                        batch_size=BATCH_SIZE, \n",
    "                        shuffle=True),\n",
    "    \"valid\": DataLoader(dataset=valid_dataset,\n",
    "                        batch_size=BATCH_SIZE, \n",
    "                        shuffle=False)    \n",
    "}\n",
    "\n",
    "\n",
    "class DistilBertForSequenceClassification(nn.Module):\n",
    "\n",
    "    def __init__(self, pretrained_model_name: str, num_classes: int = None):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        config = AutoConfig.from_pretrained(\n",
    "            pretrained_model_name, num_labels=num_classes)\n",
    "\n",
    "        self.distilbert = AutoModel.from_pretrained(pretrained_model_name,\n",
    "                                                    config=config)\n",
    "        self.pre_classifier = nn.Linear(config.dim, config.dim)\n",
    "        self.classifier = nn.Linear(config.dim, num_classes)\n",
    "        self.dropout = nn.Dropout(config.seq_classif_dropout)\n",
    "\n",
    "    def forward(self, features, attention_mask=None, head_mask=None):\n",
    "\n",
    "        assert attention_mask is not None, \"attention mask is none\"\n",
    "        distilbert_output = self.distilbert(input_ids=features,\n",
    "                                            attention_mask=attention_mask,\n",
    "                                            head_mask=head_mask)\n",
    "\n",
    "        hidden_state = distilbert_output[0]  \n",
    "        pooled_output = hidden_state[:, 0] \n",
    "        pooled_output = self.pre_classifier(pooled_output)  \n",
    "        pooled_output = nn.ReLU()(pooled_output)  \n",
    "        pooled_output = self.dropout(pooled_output)  \n",
    "        logits = self.classifier(pooled_output)  \n",
    "\n",
    "        return logits\n",
    "\n",
    "model = DistilBertForSequenceClassification(pretrained_model_name=MODEL_NAME,\n",
    "                                            num_classes=NUM_CLASSES)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARN_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YEwYLBtGrEPV",
    "outputId": "5d0a5d3c-e268-4d03-f576-172047bf2795"
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8oc1rhp3rEPY"
   },
   "outputs": [],
   "source": [
    "PATH=\"./export_model/state_dict\"\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eTAbmOC_rEPc",
    "outputId": "0bd700d0-d157-46d9-b61b-7cbe6a7c85cb"
   },
   "outputs": [],
   "source": [
    "torch.load(\"news-classification/checkpoints/best_full.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J-cinO5mrEPm",
    "outputId": "5f24756e-dc2c-483c-d461-4a74bcc74771"
   },
   "outputs": [],
   "source": [
    "!ls news-classification/checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Z9iHt7YrEPq"
   },
   "outputs": [],
   "source": [
    "PATH=\"./export_model/export\"\n",
    "torch.save(model, PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b7KnukRJ5yiA"
   },
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"   \n",
    "set_global_seed(SEED)                       \n",
    "prepare_cudnn(deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "id": "gZW5gfph5157",
    "outputId": "a41a734d-eed2-4e8f-c7e1-39d648344424"
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "runner = SupervisedRunner(input_key=(\"features\",\"attention_mask\"))\n",
    "\n",
    "runner.train(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    loaders=train_val_loaders,\n",
    "    callbacks=[\n",
    "        AccuracyCallback(num_classes=NUM_CLASSES),\n",
    "#       F1ScoreCallback(activation='Softmax'), # Tried it, but got an error on tensor shape\n",
    "        OptimizerCallback(accumulation_steps=ACCUM_STEPS)\n",
    "    ],\n",
    "    fp16=FP16_PARAMS,\n",
    "    logdir=LOG_DIR,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    verbose=True,\n",
    "    #resume=LOG_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZTZ0TtaLrEPy"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "import boto3\n",
    "\n",
    "sm = boto3.client('sagemaker')\n",
    "sm.stop_notebook_instance(NotebookInstanceName='string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 32
    },
    "id": "4_Y_HAv3gZtW",
    "outputId": "d8f93045-b467-48a3-fa6f-e37bf8f2657b"
   },
   "outputs": [],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s53UMZE0fDf-"
   },
   "outputs": [],
   "source": [
    "!cp news-classification/checkpoints/train.3.pth \"/gdrive/My Drive/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 65
    },
    "id": "uHbLOHec53hh",
    "outputId": "c8abacb5-eb33-4f69-bede-e61795a0d530"
   },
   "outputs": [],
   "source": [
    "test_loaders = {\n",
    "    \"test\": DataLoader(dataset=test_dataset,\n",
    "                        batch_size=BATCH_SIZE, \n",
    "                        shuffle=False) \n",
    "}\n",
    "\n",
    "runner.infer(\n",
    "    model=model,\n",
    "    loaders=test_loaders,\n",
    "    callbacks=[\n",
    "        CheckpointCallback(\n",
    "            resume=f\"{LOG_DIR}/checkpoints/best.pth\"\n",
    "        ),\n",
    "        InferCallback(),\n",
    "    ],   \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 745
    },
    "id": "UNhvBOeV55t4",
    "outputId": "60ef0250-e0c3-4975-a19e-a0e40894e581"
   },
   "outputs": [],
   "source": [
    "predicted_probs = runner.state.callbacks[0].predictions['logits']\n",
    "test_pred = pd.DataFrame(\n",
    "    {\n",
    "        \"label\": predicted_probs.argmax(axis=1)\n",
    "    }\n",
    ")\n",
    "test_pred[\"label\"] = test_pred[\"label\"].map({v:k for k, v in train_dataset.label_dict.items()})\n",
    "\n",
    "unique_label = train[\"label\"].unique()\n",
    "cmtx = pd.DataFrame(\n",
    "    confusion_matrix(test_true, test_pred, labels=unique_label), \n",
    "    index=['{:}'.format(x) for x in unique_label], \n",
    "    columns=['{:}'.format(x) for x in unique_label]\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "ax = sns.heatmap(cmtx, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\n",
    "ax.set_ylabel('Predicted')\n",
    "ax.set_xlabel('Target');\n",
    "ax.set_title(\"Figure 9 - Test Set Confusion Matrix\", size=12)\n",
    "\n",
    "acc = accuracy_score(test_true, test_pred)\n",
    "prec = precision_score(test_true, test_pred, average=\"weighted\")\n",
    "f1 = f1_score(test_true, test_pred, average=\"weighted\")\n",
    "print(f\"Test Accuracy: {acc}\")\n",
    "print(f\"Test Precision: {prec}\")\n",
    "print(f\"Test F1 Score: {f1}\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 352
    },
    "id": "xqD_lvPdP9Kn",
    "outputId": "eaf825ad-0184-4bf3-98ff-612044f5accb"
   },
   "outputs": [],
   "source": [
    "!pip install hackernews-client -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0rJj3UI9NAkI"
   },
   "outputs": [],
   "source": [
    "from hackernews import hn\n",
    "news_client = hn.NewsClient()\n",
    "\n",
    "def get_comm(x):\n",
    "  try:\n",
    "    c_id = x.kids[0]\n",
    "    return requests.get(f\"https://hacker-news.firebaseio.com/v0/item/{c_id}.json?print=pretty\").json()['text']\n",
    "  except:\n",
    "    return \"\"\n",
    "\n",
    "h = news_client.get_best_story(fetchMax=200)\n",
    "\n",
    "headline_and_comment  =  [x.title + \" \" + get_comm(x) for x in h]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 592
    },
    "id": "sNgejinsLFcK",
    "outputId": "bb047016-2bca-4ce2-8377-96cc369820ae"
   },
   "outputs": [],
   "source": [
    "test_dataset = TextClassificationDataset(\n",
    "    texts=headline_and_comment,\n",
    "    labels=None,\n",
    "    label_dict=None,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    model_name=MODEL_NAME\n",
    ")\n",
    "\n",
    "test_loaders = {\n",
    "    \"test\": DataLoader(dataset=test_dataset,\n",
    "                        batch_size=BATCH_SIZE, \n",
    "                        shuffle=False) \n",
    "}\n",
    "\n",
    "runner.infer(\n",
    "    model=model,\n",
    "    loaders=test_loaders,\n",
    "    callbacks=[\n",
    "        CheckpointCallback(\n",
    "            resume=f\"{LOG_DIR}/checkpoints/best.pth\"\n",
    "        ),\n",
    "        InferCallback(),\n",
    "    ],   \n",
    "    verbose=True\n",
    ")\n",
    "predicted_probs = runner.state.callbacks[0].predictions['logits']\n",
    "test_pred = pd.DataFrame(\n",
    "    {\n",
    "        \"headline\" : headline_and_comment,\n",
    "        \"label\": pd.Series(np.argmax(predicted_probs,axis=1)).map({v:k for k, v in train_dataset.label_dict.items()})\n",
    "    }\n",
    ")\n",
    "\n",
    "test_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 657
    },
    "id": "Ozn2xxbQSuW6",
    "outputId": "ffd4779f-5350-4551-8d4a-a32c89463fcd"
   },
   "outputs": [],
   "source": [
    "test_pred.query(\"label=='BUSINESS'\").head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 657
    },
    "id": "6ullr5agNbrN",
    "outputId": "340c46d3-c40f-4301-eb0f-6a8a77456504"
   },
   "outputs": [],
   "source": [
    "test_pred.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qbP7i69QS6NI"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Topic Modelic.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}